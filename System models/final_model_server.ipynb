{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1AwbTDFV8BA",
        "outputId": "11eb96a1-f5fa-4cee-c31f-4c160abc75dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Get weights and needed files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0VaRbQF5qqg",
        "outputId": "bc96d514-f68a-43f7-e686-5efcbf401046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6C63O4iC_KU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import glob\n",
        "import torch\n",
        "import gdown\n",
        "import argparse\n",
        "import statistics\n",
        "import threading\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import albumentations as A\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import *\n",
        "from base64 import b64encode\n",
        "from collections import deque\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhYMsNhz8T7Y",
        "outputId": "f14bc105-71d9-4f93-8f99-89b929aed061"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MC3_18_Weights.KINETICS400_V1`. You can also use `weights=MC3_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "Downloading: \"https://download.pytorch.org/models/mc3_18-a90a0ba3.pth\" to /root/.cache/torch/hub/checkpoints/mc3_18-a90a0ba3.pth\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "predicted_class_name = \"\"\n",
        "DATASET_DIR = ''\n",
        "CLASSES_LIST = ['fight','noFight']\n",
        "SEQUENCE_LENGTH = 16\n",
        "\n",
        "\n",
        "# url = 'https://drive.google.com/uc?id=1MWDeLnpEaZDrKK-OjmzvYLxfjwp-GDcp'\n",
        "# output = 'model_16_m3_0.8888.pth'\n",
        "# gdown.download(url, output, quiet=False)\n",
        "\n",
        "output = '/content/drive/MyDrive/TheProject/Fight_Detection_From_Surveillance_Cameras-PyTorch_Project/Models/model_16_m3_0.8888.pth'\n",
        "__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(output)))\n",
        "modelPath= os.path.join(__location__, 'model_16_m3_0.8888.pth')\n",
        "###############################################################################\n",
        "\n",
        "# Define the transforms\n",
        "def transform_():\n",
        "    transform = A.Compose(\n",
        "    [A.Resize(128, 171, always_apply=True),A.CenterCrop(112, 112, always_apply=True),\n",
        "     A.Normalize(mean = [0.43216, 0.394666, 0.37645],std = [0.22803, 0.22145, 0.216989], always_apply=True)]\n",
        "     )\n",
        "    return transform\n",
        "\n",
        "\n",
        "def frames_extraction(video_path,SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "        SEQUENCE_LENGTH: TThe number of Frames we want.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store video frames.\n",
        "    frames_list = []\n",
        "\n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    transform= transform_()\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video.\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        # Check if Video frame is not successfully read then break the loop\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        image = frame.copy()\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = transform(image=frame)['image']\n",
        "\n",
        "        # Append the normalized frame into the frames list\n",
        "        frames_list.append(frame)\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return frames_list\n",
        "\n",
        "\n",
        "def create_dataset(DATASET_DIR,CLASSES_LIST,SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "    '''\n",
        "\n",
        "    # Declared Empty Lists to store the features and labels.\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterating through all the classes mentioned in the classes list\n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "\n",
        "        # Display the name of the class whose data is being extracted.\n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "\n",
        "        # Get the list of video files present in the specific class name directory.\n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "\n",
        "        # Iterate through all the files present in the files list.\n",
        "        for file_name in files_list:\n",
        "\n",
        "            # Get the complete video path.\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "\n",
        "            # Extract the frames of the video file.\n",
        "            frames = frames_extraction(video_file_path,SEQUENCE_LENGTH)\n",
        "\n",
        "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "                # Append the data to their repective lists.\n",
        "                input_frames = np.array(frames)\n",
        "\n",
        "                # transpose to get [3, num_clips, height, width]\n",
        "                input_frames = np.transpose(input_frames, (3,0, 1, 2))\n",
        "\n",
        "                # convert the Frames & Labels to tensor\n",
        "                input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
        "                label = torch.tensor(int(class_index))\n",
        "\n",
        "                # Append the data to their repective lists and Stack them as Tensor.\n",
        "                features.append(input_frames) # append to list\n",
        "                labels.append(label) # append to list\n",
        "\n",
        "\n",
        "\n",
        "    # Return the frames, class index, and video file path.\n",
        "    return  torch.stack(features), torch.stack(labels)\n",
        "\n",
        "# Function To Train the Model From Pytorch Documentation\n",
        "def train_model(device,model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n",
        "\n",
        "def loadModel():\n",
        "  PATH=modelPath\n",
        "  model_ft = torchvision.models.video.mc3_18(pretrained=True, progress=False)\n",
        "  num_ftrs = model_ft.fc.in_features         #in_features\n",
        "  model_ft.fc = torch.nn.Linear(num_ftrs, 2) #nn.Linear(in_features, out_features)\n",
        "  model_ft.load_state_dict(torch.load(PATH,map_location=torch.device(device)))\n",
        "  model_ft.to(device)\n",
        "  model_ft.eval()\n",
        "  return model_ft\n",
        "\n",
        "model = loadModel()\n",
        "\n",
        "def PredTopKClass(k, clips):\n",
        "  with torch.no_grad(): # we do not want to backprop any gradients\n",
        "\n",
        "      input_frames = np.array(clips)\n",
        "\n",
        "      # add an extra dimension\n",
        "      input_frames = np.expand_dims(input_frames, axis=0)\n",
        "\n",
        "      # transpose to get [1, 3, num_clips, height, width]\n",
        "      input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
        "\n",
        "      # convert the frames to tensor\n",
        "      input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
        "      input_frames = input_frames.to(device)\n",
        "\n",
        "      # forward pass to get the predictions\n",
        "      outputs = model(input_frames)\n",
        "\n",
        "      # get the prediction index\n",
        "      soft_max = torch.nn.Softmax(dim=1)\n",
        "      probs = soft_max(outputs.data)\n",
        "      prob, indices = torch.topk(probs, k)\n",
        "\n",
        "  Top_k = indices[0]\n",
        "  Classes_nameTop_k=[CLASSES_LIST[item].strip() for item in Top_k]\n",
        "  ProbTop_k=prob[0].tolist()\n",
        "  ProbTop_k = [round(elem, 5) for elem in ProbTop_k]\n",
        "  return Classes_nameTop_k[0]     #list(zip(Classes_nameTop_k,ProbTop_k))\n",
        "\n",
        "\n",
        "def PredTopKProb(k,clips):\n",
        "  with torch.no_grad(): # we do not want to backprop any gradients\n",
        "\n",
        "      input_frames = np.array(clips)\n",
        "\n",
        "      # add an extra dimension\n",
        "      input_frames = np.expand_dims(input_frames, axis=0)\n",
        "\n",
        "      # transpose to get [1, 3, num_clips, height, width]\n",
        "      input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
        "\n",
        "      # convert the frames to tensor\n",
        "      input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
        "      input_frames = input_frames.to(device)\n",
        "\n",
        "      # forward pass to get the predictions\n",
        "      outputs = model(input_frames)\n",
        "\n",
        "      # get the prediction index\n",
        "      soft_max = torch.nn.Softmax(dim=1)\n",
        "      probs = soft_max(outputs.data)\n",
        "      prob, indices = torch.topk(probs, k)\n",
        "\n",
        "  Top_k = indices[0]\n",
        "  Classes_nameTop_k=[CLASSES_LIST[item].strip() for item in Top_k]\n",
        "  ProbTop_k=prob[0].tolist()\n",
        "  ProbTop_k = [round(elem, 5) for elem in ProbTop_k]\n",
        "  return list(zip(Classes_nameTop_k,ProbTop_k))\n",
        "\n",
        "def downloadYouTube(videourl, path):\n",
        "\n",
        "    yt = YouTube(videourl)\n",
        "    yt = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    yt.download(path)\n",
        "\n",
        "def show_video(file_name, width=640):\n",
        "  # show resulting deepsort video\n",
        "  mp4 = open(file_name,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"\n",
        "  <video width=\"{0}\" controls>\n",
        "        <source src=\"{1}\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\".format(width, data_url))\n",
        "\n",
        "def FightInference(video_path,SEQUENCE_LENGTH=64):\n",
        "  clips = frames_extraction(video_path,SEQUENCE_LENGTH)\n",
        "  print(PredTopKClass(1,clips))\n",
        "  print(PredTopKProb(2,clips))\n",
        "  return \"***********\"\n",
        "\n",
        "\n",
        "def FightInference_Time(video_path,SEQUENCE_LENGTH=64):\n",
        "  start_time = time.time()\n",
        "  clips = frames_extraction(video_path,SEQUENCE_LENGTH)\n",
        "  class_=PredTopKClass(1,clips)\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"time is:\",elapsed)\n",
        "  return class_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_on_video(video_file_path, output_file_path, image_path,SEQUENCE_LENGTH,skip=2,showInfo=False):\n",
        "    '''\n",
        "    This function will perform action recognition on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n",
        "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        "    i=0\n",
        "    j=0\n",
        "    flag=0\n",
        "    # Initialize the VideoCapture object to read from the video file.\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Get the width and height of the video.\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize the VideoWriter Object to store the output video in the disk.\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    # Declare a queue to store video frames.\n",
        "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "    transform= transform_()\n",
        "    # Initialize a variable to store the predicted action being performed in the video.\n",
        "    predicted_class_name = ''\n",
        "\n",
        "    # Iterate until the video is accessed successfully.\n",
        "    counter=0\n",
        "    while video_reader.isOpened():\n",
        "\n",
        "        # Read the frame.\n",
        "        ok, frame = video_reader.read()\n",
        "\n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        image = frame.copy()\n",
        "        framee = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        framee = transform(image=framee)['image']\n",
        "        if counter % skip==0:\n",
        "          # Appending the pre-processed frame into the frames list.\n",
        "          frames_queue.append(framee)\n",
        "\n",
        "\n",
        "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
        "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
        "          predicted_class_name= PredTopKClass(1,frames_queue)\n",
        "          if showInfo:\n",
        "            print(predicted_class_name)\n",
        "            frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "          else:\n",
        "            frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "\n",
        "        # Write predicted class name on top of the frame.\n",
        "        if predicted_class_name==\"fight\":\n",
        "          if(j%30==0):\n",
        "            flag=1\n",
        "            i=i+1\n",
        "            folder_path = image_path\n",
        "            file_name = os.path.join(folder_path, f\"fight_{i}.jpg\")\n",
        "            cv2.imwrite(file_name, frame)\n",
        "          # img = recognize2(frame)\n",
        "          # cv2.imwrite(f'cina_{i}.jpg', img)\n",
        "          j=j+1\n",
        "          cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
        "        else:\n",
        "          cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        counter+=1\n",
        "\n",
        "        # Write The frame into the disk using the VideoWriter Object.\n",
        "\n",
        "        video_writer.write(frame)\n",
        "        # time.sleep(2)\n",
        "    if showInfo:\n",
        "      print(counter)\n",
        "    # Release the VideoCapture and VideoWriter objects.\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    # fight detected\n",
        "    return flag\n",
        "\n",
        "def fightDetection(inputPath,seq,skip,outputPath,image_path,showInfo=False):\n",
        "\n",
        "    # Perform Accident Detection on the Test Video.\n",
        "    return predict_on_video(inputPath, outputPath,image_path,seq,skip,showInfo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ProFFbU9s5J",
        "outputId": "f288662f-3ef3-4d3d-e0a7-71eb10860472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Take: 0.00029945373535156250  Second\n",
            "Take: 0.00000499089558919271  min\n"
          ]
        }
      ],
      "source": [
        "start=time.time()\n",
        "frame_count = 0\n",
        "def fight_detection(input_path, output_path, image_folder, input_criminal_face, output_face_folder):\n",
        "    global frame_count\n",
        "    flag =  fightDetection(input_path,16,2,output_path,image_folder,False)\n",
        "    flag=0\n",
        "    print(\"flag = {}\".format(flag))\n",
        "    if flag==1:\n",
        "      image_files = os.listdir(image_folder)\n",
        "      for image_file in image_files:\n",
        "          frame_count += 1\n",
        "          path = os.path.join(image_folder,image_file)\n",
        "          print(path)\n",
        "          image = face_recognition.load_image_file(path)\n",
        "          # image = cv2.imread(path)\n",
        "\n",
        "          # # Convert the image to RGB (face_recognition uses RGB images)\n",
        "          # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "          recognize2(image, input_criminal_face, output_face_folder)\n",
        "\n",
        "      #   # Open the video file\n",
        "      #   video = cv2.VideoCapture(output_path)\n",
        "      #  # Loop through the video frames\n",
        "      #   while True:\n",
        "      #       ret, frame = video.read()\n",
        "      #       if not ret:\n",
        "      #           break\n",
        "      #       frame_count += 1\n",
        "      #       img=recognize2(frame, input_criminal_face, output_face_folder)\n",
        "      #   # Release the video object\n",
        "      #   video.release()\n",
        "\n",
        "elapsed=time.time()-start\n",
        "print(\"Take:\",f'{elapsed:.20f}',\" Second\")\n",
        "print(\"Take:\",f'{elapsed/60:.20f}',\" min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPx8Z95U4YBl",
        "outputId": "9c5d2b76-0918-471b-c4e2-2fa43bc8f663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.3)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=6acfc2d22fe17ef0a105d0f7841c61df4c53456dd32dea6eebd07075047cb32a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yri7ZyuG4YD1"
      },
      "outputs": [],
      "source": [
        "import face_recognition\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahr9tTZSDOwm"
      },
      "source": [
        "###Graduation Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo_4X8NaDR_i",
        "outputId": "c029632f-a979-4ab2-9aec-93af49e6806c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def recognize4(img, encoded_faces, names):\n",
        "    faces_locations = face_recognition.face_locations(img)\n",
        "    faces_encodings = face_recognition.face_encodings(img)\n",
        "    detected = False\n",
        "\n",
        "    for img_encoding, face_location in zip(faces_encodings, faces_locations):\n",
        "        results = face_recognition.compare_faces(encoded_faces, img_encoding, tolerance=0.5)\n",
        "        detected = any(results)\n",
        "\n",
        "        if detected:\n",
        "            matched_name = \"Unknown\"\n",
        "\n",
        "            # Find the name associated with the matched face\n",
        "            for idx, result in enumerate(results):\n",
        "                if result:\n",
        "                    matched_name = names[idx]\n",
        "                    break\n",
        "\n",
        "            # Increase the box size\n",
        "            change = 7\n",
        "            face_top = max(0, face_location[0] - change)\n",
        "            face_right = min(img.shape[1], face_location[1] + change)\n",
        "            face_bottom = min(img.shape[0], face_location[2] + change)\n",
        "            face_left = max(0, face_location[3] - change)\n",
        "\n",
        "            img = cv2.putText(img, matched_name, (face_left, face_top - change), cv2.FONT_HERSHEY_DUPLEX, 0.7, (255, 255, 255), 2)\n",
        "            img = cv2.rectangle(img, (face_left, face_top), (face_right, face_bottom), (255, 255, 255), 2)\n",
        "\n",
        "            # img = cv2.putText(img, matched_name, (face_location[3], face_location[0]-20), cv2.FONT_HERSHEY_DUPLEX, 1.0, (255, 255, 255), 2)\n",
        "            # img = cv2.rectangle(img, (face_location[3], face_location[0]), (face_location[1], face_location[2]), (255, 255, 255), 2)\n",
        "\n",
        "    return img, detected\n",
        "our_img_folder = '/content/drive/MyDrive/TheProject/our_img_folder'\n",
        "list_our_images = os.listdir(our_img_folder)\n",
        "image_names = [os.path.splitext(file)[0] for file in list_our_images]\n",
        "criminal_images = [face_recognition.load_image_file(os.path.join(our_img_folder, image_name)) for image_name in list_our_images]\n",
        "encoded_faces = [face_recognition.face_encodings(image)[0] for image in criminal_images]\n",
        "print(len(encoded_faces))\n",
        "\n",
        "img_num =1\n",
        "path = f'/content/drive/MyDrive/TheProject/grad/grad{img_num}.jpg'\n",
        "img = cv2.imread(path)\n",
        "# Call the recognize3 function with the updated arguments\n",
        "img_result, is_detected = recognize4(img, encoded_faces, image_names)\n",
        "result_path = '/content/drive/MyDrive/TheProject/grad_result'\n",
        "cv2.imwrite(os.path.join(result_path, f'result{img_num}.jpg'), img_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAtyi4xP4YGE"
      },
      "outputs": [],
      "source": [
        "def recognize2(img, input_criminal_face, output_face_folder):\n",
        "    cina_image = face_recognition.load_image_file(input_criminal_face) #john cina photo\n",
        "    cina_encoding = face_recognition.face_encodings(cina_image)[0]\n",
        "    print('here')\n",
        "    faces_locations = face_recognition.face_locations(img)\n",
        "    print('here2')\n",
        "    faces_encodings = face_recognition.face_encodings(img)\n",
        "    print('here3')\n",
        "    for img_encoding , face_location  in zip(faces_encodings , faces_locations):\n",
        "        results = face_recognition.compare_faces([cina_encoding], img_encoding , tolerance = 0.6)\n",
        "        if sum(results)>0:\n",
        "                folder_path = output_face_folder\n",
        "                file_name = os.path.join(folder_path, f\"face_{frame_count}.jpg\")\n",
        "                print('yes')\n",
        "                img = cv2.putText(img, 'cina' , (face_location[3],face_location[0]-20), cv2.FONT_HERSHEY_DUPLEX, 2.0, (255,255,255), 2)\n",
        "                img = cv2.rectangle(img, (face_location[3],face_location[0]), (face_location[1],face_location[2]) , (255,255,255), 2)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                cv2.imwrite(file_name, img)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEul3eA5cNy"
      },
      "source": [
        "#The fight and face together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSjbpZsyh5nu"
      },
      "source": [
        "###Fight and face models together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlKQxCiLmXLF"
      },
      "outputs": [],
      "source": [
        "def recognize3(img, encoded_face):\n",
        "    faces_locations = face_recognition.face_locations(img)\n",
        "    faces_encodings = face_recognition.face_encodings(img)\n",
        "    detected = False\n",
        "    for img_encoding , face_location  in zip(faces_encodings , faces_locations):\n",
        "        results = face_recognition.compare_faces(encoded_face, img_encoding , tolerance = 0.5)\n",
        "        detected = sum(results)>0\n",
        "        if detected:\n",
        "                # folder_path = output_face_folder\n",
        "                # file_name = os.path.join(folder_path, f\"face_{frame_count}.jpg\")\n",
        "                img = cv2.putText(img, 'Criminal' , (face_location[3],face_location[0]-20), cv2.FONT_HERSHEY_DUPLEX, 1.0, (255,255,255), 2)\n",
        "                img = cv2.rectangle(img, (face_location[3],face_location[0]), (face_location[1],face_location[2]) , (255,255,255), 2)\n",
        "                # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                # cv2.imwrite(file_name, img)\n",
        "    return img, detected\n",
        "# def predict_on_video(video_file_path, output_file_path, image_path,SEQUENCE_LENGTH,skip=2,showInfo=False):\n",
        "def fightDetection2(video_file_path,output_file_path,image_path,input_criminal_folder,list_criminal_images,ouput_face_folder,face_model=False,SEQUENCE_LENGTH=16,skip=2,showInfo=False):\n",
        "    '''\n",
        "    This function will perform action recognition on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n",
        "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        "    i=0\n",
        "    j=0\n",
        "    k=0\n",
        "    flag=0\n",
        "\n",
        "    global frame_count\n",
        "    global frame_number\n",
        "\n",
        "    # Initialize the VideoCapture object to read from the video file.\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    frame_number = 0\n",
        "    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Get the width and height of the video.\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize the VideoWriter Object to store the output video in the disk.\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    # Declare a queue to store video frames.\n",
        "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "    transform= transform_()\n",
        "    # Initialize a variable to store the predicted action being performed in the video.\n",
        "    predicted_class_name = ''\n",
        "\n",
        "    # Iterate until the video is accessed successfully.\n",
        "    counter=0\n",
        "\n",
        "    # criminal_image = face_recognition.load_image_file(input_criminal_face) #john cina photo\n",
        "    # criminal_encoding = face_recognition.face_encodings(criminal_image)[0]\n",
        "    if face_model:\n",
        "        criminal_images = [face_recognition.load_image_file(os.path.join(input_criminal_folder, image_name)) for image_name in list_criminal_images]\n",
        "        # criminal_encodings = [face_recognition.face_encodings(image)[0] for image in criminal_images]\n",
        "        criminal_encodings = []\n",
        "        for image in criminal_images:\n",
        "            face_encodings = face_recognition.face_encodings(image)\n",
        "            if len(face_encodings) > 0:\n",
        "                criminal_encodings.append(face_encodings[0])\n",
        "\n",
        "        if not (len(criminal_encodings) > 0):\n",
        "            face_model = False\n",
        "        print(\"************\")\n",
        "        print(\"Number of face encodings: \", len(criminal_encodings))\n",
        "        print(\"************\")\n",
        "    print(face_model)\n",
        "    while video_reader.isOpened():\n",
        "\n",
        "        # Read the frame.\n",
        "        ok, frame = video_reader.read()\n",
        "\n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        image = frame.copy()\n",
        "        framee = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        framee = transform(image=framee)['image']\n",
        "        if counter % skip==0:\n",
        "          # Appending the pre-processed frame into the frames list.\n",
        "          frames_queue.append(framee)\n",
        "\n",
        "\n",
        "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
        "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
        "          predicted_class_name= PredTopKClass(1,frames_queue)\n",
        "          if showInfo:\n",
        "            print(predicted_class_name)\n",
        "            frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "          else:\n",
        "            frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "\n",
        "        # Write predicted class name on top of the frame.\n",
        "        if predicted_class_name==\"fight\":\n",
        "          if(j%30==0):\n",
        "            flag=1\n",
        "            i=i+1\n",
        "            folder_path = image_path\n",
        "            file_name = os.path.join(folder_path, f\"fight_{i}.jpg\")\n",
        "            cv2.imwrite(file_name, frame)\n",
        "          # img = recognize2(frame)\n",
        "          # cv2.imwrite(f'cina_{i}.jpg', img)\n",
        "\n",
        "          j=j+1\n",
        "          cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
        "        else:\n",
        "          cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        counter+=1\n",
        "\n",
        "        if face_model:\n",
        "            frame,face_detected = recognize3(frame,criminal_encodings)\n",
        "            if (face_detected) and (frame_number%15==0):\n",
        "                k=k+1\n",
        "                folder_path = ouput_face_folder\n",
        "                file_name = os.path.join(folder_path, f\"face_{k}.jpg\")\n",
        "                cv2.imwrite(file_name, frame)\n",
        "\n",
        "        # Write The frame into the disk using the VideoWriter Object.\n",
        "\n",
        "        video_writer.write(frame)\n",
        "        frame_number += 1\n",
        "\n",
        "        # time.sleep(2)\n",
        "    if showInfo:\n",
        "      print(counter)\n",
        "    # Release the VideoCapture and VideoWriter objects.\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    # fight detected\n",
        "    return flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9XhA57QHZc9"
      },
      "source": [
        "###Car crash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87PdzwwjHljI"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "weights_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3.weights\")\n",
        "cfg_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3.cfg.txt\")\n",
        "net=cv2.dnn.readNetFromDarknet(cfg_path ,weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwuJrsA4Ag_3"
      },
      "outputs": [],
      "source": [
        "def car_accident(img) :\n",
        "  (H,W)=img.shape[:2]\n",
        "  blob= cv2.dnn.blobFromImage(img,1/255.0,(416,416),crop=False,swapRB=False)\n",
        "  net.setInput(blob)\n",
        "  detected = ''\n",
        "  layers_output=net.forward(['yolo_82', 'yolo_94', 'yolo_106'])\n",
        "  car_boxes=[]\n",
        "  car_confidences=[]\n",
        "  classIDs=[]\n",
        "  for output in layers_output:\n",
        "    for detection in output:\n",
        "        scores=detection[5:]\n",
        "        classID=np.argmax(scores)\n",
        "        confidence=scores[classID]\n",
        "        if classID==2:\n",
        "          if (confidence >0.5):\n",
        "            box=detection[:4] * np.array([W,H,W,H])\n",
        "            bx,by,bw,bh=box.astype(\"int\")\n",
        "            x=int(bx-(bw/2))\n",
        "            y=int(by-(bh/2))\n",
        "            car_boxes.append([x,y,int(bw),int(bh)])\n",
        "            car_confidences.append(float(confidence))\n",
        "            classIDs.append(classID)\n",
        "  car_idx=cv2.dnn.NMSBoxes(car_boxes,car_confidences,0.5,0.4)\n",
        "\n",
        "  if len(car_idx) > 0:  \t\t\t\t\t\t# At least 1 detection in the image and check detection presence in a frame\n",
        "        centroid_dict = dict() \t\t\t\t\t\t# Function creates a dictionary and calls it centroid_dict\n",
        "        objectId = 0\t\t\t\t\t\t\t\t# We inialize a variable called ObjectId and set it to 0\n",
        "        for i in car_idx.flatten():\n",
        "          (x,y)=[car_boxes[i][0],car_boxes[i][1]]\n",
        "          (w,h)=[car_boxes[i][2],car_boxes[i][3]]\n",
        "\n",
        "     \t    # Store the center points of the detections\n",
        "          # Convert from center coordinates to rectangular coordinates, We use floats to ensure the precision of the BBox\n",
        "          xmin = int(round(x))\n",
        "          xmax = int(xmin+w)\n",
        "          ymin = int(round(y))\n",
        "          ymax = int(ymin+h)\n",
        "\n",
        "          # Append center point of bbox for cars detected.\n",
        "          centroid_dict[objectId] = (int(x), int(y), xmin, ymin, xmax, ymax) # Create dictionary of tuple with 'objectId' as the index center points and bbox\n",
        "          objectId += 1 #Increment the index for each detection\n",
        "    #=================================================================#\n",
        "    # Purpose : Determine which car bbox are close to each other\n",
        "    #=================================================================\n",
        "        vehicle_red_zone_list = [] # List containing which Object id is in under threshold distance condition.\n",
        "        vehicle_red_line_list = []\n",
        "        for (id1, p1), (id2, p2) in combinations(centroid_dict.items(), 2): # Get all the combinations of close detections, #List of multiple items - id1 1, points 2, 1,3\n",
        "            #dx, dy = p1[0] - p2[0], p1[1] - p2[1]  \t# Check the difference between centroid x: 0, y :1\n",
        "            #distance = is_close(dx, dy) \t\t\t# Calculates the Euclidean distance\n",
        "\n",
        "            #if distance < 50.0:\t\t\t\t\t\t# Set our distance threshold - If they meet this condition then..\n",
        "\n",
        "            if not ((p1[2]+30>=p2[4]) or (p1[4]<=p2[2]+30) or (p1[5]<=p2[3]+30) or (p1[3]+30>=p2[5])):\n",
        "                if id1 not in vehicle_red_zone_list:\n",
        "                    vehicle_red_zone_list.append(id1)       #  Add Id to a list\n",
        "                    vehicle_red_line_list.append(p1[0:2])   #  Add points to the list\n",
        "                if id2 not in vehicle_red_zone_list:\n",
        "                    vehicle_red_zone_list.append(id2)\t\t# Same for the second id\n",
        "                    vehicle_red_line_list.append(p2[0:2])\n",
        "\n",
        "        for idx1, box in centroid_dict.items():  # dict (1(key):red(value), 2 blue)  idx - key  box - value\n",
        "            if idx1 in vehicle_red_zone_list:   # if id is in red zone list\n",
        "                cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (0, 0, 255), 2) # Create Red bounding boxes  #starting point, ending point size of 2\n",
        "            else:\n",
        "                cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (0, 255, 0), 2) # Create Green bounding boxes\n",
        "\t\t#=================================================================#\n",
        "\n",
        "        if len(vehicle_red_zone_list)!=0:\n",
        "            text = \"Crash Detected\"\n",
        "        else:\n",
        "            text = \"Crash Not Detected\"\n",
        "\n",
        "        location = (W-550,80)\t\t\t\t\t\t\t\t\t\t# Set the location of the displayed text\n",
        "        if len(vehicle_red_zone_list)!=0:\n",
        "            cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)  # Display Text\n",
        "            detected = 'crash'\n",
        "        else:\n",
        "            cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)  # Display Text\n",
        "  return img,detected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KDsZmB8ULJo"
      },
      "source": [
        "###Human_Fall detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUYq-1EqUSq5"
      },
      "outputs": [],
      "source": [
        "weights_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3.weights\")\n",
        "cfg_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3.cfg.txt\")\n",
        "# weights_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3-tiny.weights\")\n",
        "# cfg_path =os.path.join(\"/content/drive/MyDrive/TheProject/yolov3-tiny.cfg\")\n",
        "net=cv2.dnn.readNetFromDarknet(cfg_path ,weights_path)\n",
        "\n",
        "def human_fall(img) :\n",
        "  (H,W)=img.shape[:2]\n",
        "  blob= cv2.dnn.blobFromImage(img,1/255.0,(416,416),crop=False,swapRB=False)\n",
        "  net.setInput(blob)\n",
        "  layers_output=net.forward(['yolo_82', 'yolo_94', 'yolo_106'])\n",
        "  boxes=[]\n",
        "  confidences=[]\n",
        "  classIDs=[]\n",
        "  for output in layers_output:\n",
        "    for detection in output:\n",
        "        scores=detection[5:]\n",
        "        classID=np.argmax(scores)\n",
        "        confidence=scores[classID]\n",
        "        if classID==0 :\n",
        "          if (confidence >0.5):\n",
        "            box=detection[:4] * np.array([W,H,W,H])\n",
        "            bx,by,bw,bh=box.astype(\"int\")\n",
        "\n",
        "            x=int(bx-(bw/2))\n",
        "            y=int(by-(bh/2))\n",
        "\n",
        "            boxes.append([x,y,int(bw),int(bh)])\n",
        "            confidences.append(float(confidence))\n",
        "            classIDs.append(classID)\n",
        "  idx=cv2.dnn.NMSBoxes(boxes,confidences,0.5,0.4)\n",
        "  detected = ''\n",
        "  if len(idx)==0 :\n",
        "    return img,detected\n",
        "  else :\n",
        "      for i in idx.flatten():\n",
        "        (x,y)=[boxes[i][0],boxes[i][1]]\n",
        "        (w,h)=[boxes[i][2],boxes[i][3]]\n",
        "        if not (h-w > -100) :\n",
        "        #   cv2.rectangle (img,(x,y),(x+w ,y+h),(0,255,0),2)\n",
        "        # else :\n",
        "            detected = 'fall'\n",
        "            cv2.rectangle (img,(x,y),(x+w ,y+h),(0,0,255),2)\n",
        "            cv2.putText(img,(\"fall detected\"),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
        "  return img,detected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzIBhGu7WRMk"
      },
      "source": [
        "###Mask Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXaStKqOx_UL",
        "outputId": "deee4a11-e4bf-4d58-c4d3-76b690f1fc36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TheProject/darknet\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/TheProject/darknet/\n",
        "!chmod +x darknet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDLJ4x54y2AZ",
        "outputId": "3f4c3672-95ae-487b-9a70-043090507290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " CUDA-version: 11080 (12000), cuDNN: 8.9.0, CUDNN_HALF=1, GPU count: 1  \n",
            " CUDNN_HALF=1 \n",
            " OpenCV version: 4.2.0\n",
            " 0 : compute_capability = 750, cudnn_half = 1, GPU: Tesla T4 \n",
            "net.optimized_memory = 0 \n",
            "mini_batch = 1, batch = 1, time_steps = 1, train = 0 \n",
            "   layer   filters  size/strd(dil)      input                output\n",
            "   0 Create CUDA-stream - 0 \n",
            " Create cudnn-handle 0 \n",
            "conv     32       3 x 3/ 2    416 x 416 x   3 ->  208 x 208 x  32 0.075 BF\n",
            "   1 conv     64       3 x 3/ 2    208 x 208 x  32 ->  104 x 104 x  64 0.399 BF\n",
            "   2 conv     64       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x  64 0.797 BF\n",
            "   3 route  2 \t\t                       1/2 ->  104 x 104 x  32 \n",
            "   4 conv     32       3 x 3/ 1    104 x 104 x  32 ->  104 x 104 x  32 0.199 BF\n",
            "   5 conv     32       3 x 3/ 1    104 x 104 x  32 ->  104 x 104 x  32 0.199 BF\n",
            "   6 route  5 4 \t                           ->  104 x 104 x  64 \n",
            "   7 conv     64       1 x 1/ 1    104 x 104 x  64 ->  104 x 104 x  64 0.089 BF\n",
            "   8 route  2 7 \t                           ->  104 x 104 x 128 \n",
            "   9 max                2x 2/ 2    104 x 104 x 128 ->   52 x  52 x 128 0.001 BF\n",
            "  10 conv    128       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 128 0.797 BF\n",
            "  11 route  10 \t\t                       1/2 ->   52 x  52 x  64 \n",
            "  12 conv     64       3 x 3/ 1     52 x  52 x  64 ->   52 x  52 x  64 0.199 BF\n",
            "  13 conv     64       3 x 3/ 1     52 x  52 x  64 ->   52 x  52 x  64 0.199 BF\n",
            "  14 route  13 12 \t                           ->   52 x  52 x 128 \n",
            "  15 conv    128       1 x 1/ 1     52 x  52 x 128 ->   52 x  52 x 128 0.089 BF\n",
            "  16 route  10 15 \t                           ->   52 x  52 x 256 \n",
            "  17 max                2x 2/ 2     52 x  52 x 256 ->   26 x  26 x 256 0.001 BF\n",
            "  18 conv    256       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 256 0.797 BF\n",
            "  19 route  18 \t\t                       1/2 ->   26 x  26 x 128 \n",
            "  20 conv    128       3 x 3/ 1     26 x  26 x 128 ->   26 x  26 x 128 0.199 BF\n",
            "  21 conv    128       3 x 3/ 1     26 x  26 x 128 ->   26 x  26 x 128 0.199 BF\n",
            "  22 route  21 20 \t                           ->   26 x  26 x 256 \n",
            "  23 conv    256       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x 256 0.089 BF\n",
            "  24 route  18 23 \t                           ->   26 x  26 x 512 \n",
            "  25 max                2x 2/ 2     26 x  26 x 512 ->   13 x  13 x 512 0.000 BF\n",
            "  26 conv    512       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x 512 0.797 BF\n",
            "  27 conv    256       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x 256 0.044 BF\n",
            "  28 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF\n",
            "  29 conv     21       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x  21 0.004 BF\n",
            "  30 yolo\n",
            "[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.05\n",
            "nms_kind: greedynms (1), beta = 0.600000 \n",
            "  31 route  27 \t\t                           ->   13 x  13 x 256 \n",
            "  32 conv    128       1 x 1/ 1     13 x  13 x 256 ->   13 x  13 x 128 0.011 BF\n",
            "  33 upsample                 2x    13 x  13 x 128 ->   26 x  26 x 128\n",
            "  34 route  33 23 \t                           ->   26 x  26 x 384 \n",
            "  35 conv    256       3 x 3/ 1     26 x  26 x 384 ->   26 x  26 x 256 1.196 BF\n",
            "  36 conv     21       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x  21 0.007 BF\n",
            "  37 yolo\n",
            "[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.05\n",
            "nms_kind: greedynms (1), beta = 0.600000 \n",
            "Total BFLOPS 6.789 \n",
            "avg_outputs = 299797 \n",
            " Allocate additional workspace_size = 134.22 MB \n",
            "Loading weights from /content/drive/MyDrive/TheProject/yolov4-tiny/training/yolov4-tiny-custom_best.weights...\n",
            " seen 64, trained: 204 K-images (3 Kilo-batches_64) \n",
            "Done! Loaded 38 layers from weights-file \n",
            " Detection layer: 30 - type = 28 \n",
            " Detection layer: 37 - type = 28 \n",
            "/content/drive/MyDrive/TheProject/test_mask.jpg: Predicted in 132.467000 milli-seconds.\n",
            "without_mask: 56%\n",
            "without_mask: 36%\n",
            "with_mask: 29%\n",
            "with_mask: 46%\n",
            "without_mask: 31%\n",
            "Unable to init server: Could not connect: Connection refused\n",
            "\n",
            "(predictions:1141): Gtk-\u001b[1;33mWARNING\u001b[0m **: \u001b[34m07:54:36.337\u001b[0m: cannot open display: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run your custom detector with this command (upload an image to your google drive to test, the thresh flag sets the minimum accuracy required for object detection)\n",
        "!./darknet detector test data/obj.data cfg/yolov4-tiny-custom.cfg /content/drive/MyDrive/TheProject/yolov4-tiny/training/yolov4-tiny-custom_best.weights {'/content/drive/MyDrive/TheProject/test_mask.jpg'} -thresh 0.2\n",
        "\n",
        "# Read the resulting image after detection\n",
        "result_image = cv2.imread('predictions.jpg')\n",
        "cv2.imwrite('/content/mask_tested.jpg', result_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CLBkF8wDtt"
      },
      "outputs": [],
      "source": [
        "def mask_detection(frame):\n",
        "    # Convert the frame to the required format (e.g., save it as an image file)\n",
        "    cv2.imwrite('frame.jpg', frame)\n",
        "\n",
        "    # run your custom detector with this command (upload an image to your google drive to test, the thresh flag sets the minimum accuracy required for object detection)\n",
        "    !./darknet detector test data/obj.data cfg/yolov4-tiny-custom.cfg /content/drive/MyDrive/TheProject/yolov4-tiny/training/yolov4-tiny-custom_best.weights {'frame.jpg'} -thresh 0.2 > /dev/null 2>&1\n",
        "\n",
        "    # Read the resulting image after detection\n",
        "    result_image = cv2.imread('predictions.jpg')\n",
        "\n",
        "    # Return the processed frame\n",
        "    return result_image,''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGLGJHsC3BZG"
      },
      "source": [
        "##Crash Fall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSU_DvK82zEh"
      },
      "outputs": [],
      "source": [
        "def fall_crash(img) :\n",
        "  (H,W)=img.shape[:2]\n",
        "  blob= cv2.dnn.blobFromImage(img,1/255.0,(416,416),crop=False,swapRB=False)\n",
        "  net.setInput(blob)\n",
        "  crash_detected = False\n",
        "  fall_detected = False\n",
        "  layers_output=net.forward(['yolo_82', 'yolo_94', 'yolo_106'])\n",
        "  car_boxes=[]\n",
        "  car_confidences=[]\n",
        "  classIDs=[]\n",
        "  for output in layers_output:\n",
        "    for detection in output:\n",
        "        scores=detection[5:]\n",
        "        classID=np.argmax(scores)\n",
        "        confidence=scores[classID]\n",
        "        if classID==0 or classID==2:\n",
        "          if (confidence >0.5):\n",
        "            box=detection[:4] * np.array([W,H,W,H])\n",
        "            bx,by,bw,bh=box.astype(\"int\")\n",
        "            x=int(bx-(bw/2))\n",
        "            y=int(by-(bh/2))\n",
        "            car_boxes.append([x,y,int(bw),int(bh)])\n",
        "            car_confidences.append(float(confidence))\n",
        "            classIDs.append(classID)\n",
        "  car_idx=cv2.dnn.NMSBoxes(car_boxes,car_confidences,0.5,0.4)\n",
        "\n",
        "  if len(car_idx) > 0:  \t\t\t\t\t\t# At least 1 detection in the image and check detection presence in a frame\n",
        "    centroid_dict = dict() \t\t\t\t\t\t# Function creates a dictionary and calls it centroid_dict\n",
        "    objectId = 0\t\t\t\t\t\t\t\t# We inialize a variable called ObjectId and set it to 0\n",
        "    for i in car_idx.flatten():\n",
        "        (x,y)=[car_boxes[i][0],car_boxes[i][1]]\n",
        "        (w,h)=[car_boxes[i][2],car_boxes[i][3]]\n",
        "\n",
        "        # fall\n",
        "        if classIDs[i] == 0:\n",
        "            if not(h-w > -100) :\n",
        "                # cv2.rectangle (img,(x,y),(x+w ,y+h),(0,255,0),2)\n",
        "            # else:\n",
        "                fall_detected = True\n",
        "                cv2.rectangle (img,(x,y),(x+w ,y+h),(0,0,255),2)\n",
        "                cv2.putText(img,(\"fall detected\"),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
        "        # car\n",
        "        else:\n",
        "            # Store the center points of the detections\n",
        "            # Convert from center coordinates to rectangular coordinates, We use floats to ensure the precision of the BBox\n",
        "            xmin = int(round(x))\n",
        "            xmax = int(xmin+w)\n",
        "            ymin = int(round(y))\n",
        "            ymax = int(ymin+h)\n",
        "\n",
        "            # Append center point of bbox for cars detected.\n",
        "            centroid_dict[objectId] = (int(x), int(y), xmin, ymin, xmax, ymax) # Create dictionary of tuple with 'objectId' as the index center points and bbox\n",
        "            objectId += 1 #Increment the index for each detection\n",
        "#=================================================================#\n",
        "# Purpose : Determine which car bbox are close to each other\n",
        "#=================================================================\n",
        "    vehicle_red_zone_list = [] # List containing which Object id is in under threshold distance condition.\n",
        "    vehicle_red_line_list = []\n",
        "    for (id1, p1), (id2, p2) in combinations(centroid_dict.items(), 2): # Get all the combinations of close detections, #List of multiple items - id1 1, points 2, 1,3\n",
        "        #dx, dy = p1[0] - p2[0], p1[1] - p2[1]  \t# Check the difference between centroid x: 0, y :1\n",
        "        #distance = is_close(dx, dy) \t\t\t# Calculates the Euclidean distance\n",
        "\n",
        "        #if distance < 50.0:\t\t\t\t\t\t# Set our distance threshold - If they meet this condition then..\n",
        "\n",
        "        if not ((p1[2]+30>=p2[4]) or (p1[4]<=p2[2]+30) or (p1[5]<=p2[3]+30) or (p1[3]+30>=p2[5])):\n",
        "            if id1 not in vehicle_red_zone_list:\n",
        "                vehicle_red_zone_list.append(id1)       #  Add Id to a list\n",
        "                vehicle_red_line_list.append(p1[0:2])   #  Add points to the list\n",
        "            if id2 not in vehicle_red_zone_list:\n",
        "                vehicle_red_zone_list.append(id2)\t\t# Same for the second id\n",
        "                vehicle_red_line_list.append(p2[0:2])\n",
        "\n",
        "    for idx1, box in centroid_dict.items():  # dict (1(key):red(value), 2 blue)  idx - key  box - value\n",
        "        if idx1 in vehicle_red_zone_list:   # if id is in red zone list\n",
        "            cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (0, 0, 255), 2) # Create Red bounding boxes  #starting point, ending point size of 2\n",
        "        else:\n",
        "            cv2.rectangle(img, (box[2], box[3]), (box[4], box[5]), (0, 255, 0), 2) # Create Green bounding boxes\n",
        "    #=================================================================#\n",
        "\n",
        "    if len(vehicle_red_zone_list)!=0:\n",
        "        text = \"Crash Detected\"\n",
        "    else:\n",
        "        text = \"Crash Not Detected\"\n",
        "\n",
        "    location = (W-350,30)\t\t\t\t\t\t\t\t\t\t# Set the location of the displayed text\n",
        "    if len(vehicle_red_zone_list)!=0:\n",
        "        cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)  # Display Text\n",
        "        crash_detected = True\n",
        "    else:\n",
        "        cv2.putText(img, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)  # Display Text\n",
        "\n",
        "  result = \"false\" if not(fall_detected or crash_detected) else \"crash\" if crash_detected else \"fall\"\n",
        "  return img,result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BXyPhopwEbM"
      },
      "outputs": [],
      "source": [
        "def model_call(func,input_path,output_path,fall_image_folder,crash_folder,mask_model=False):\n",
        "    i = 1\n",
        "    j = 1\n",
        "\n",
        "    global frame_count\n",
        "    global frame_number\n",
        "\n",
        "    video = cv2.VideoCapture(input_path)\n",
        "\n",
        "    frame_number = 0\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Get video properties\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Create VideoWriter object to save the processed frames as video\n",
        "    output_video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), fps, (frame_width, frame_height)) #cv2.VideoWriter_fourcc(*'mp4v'), isColor=False\n",
        "\n",
        "    # Iterate over the video frames\n",
        "    while True:\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Process the frame using the function\n",
        "        frame,detected = func(frame)\n",
        "        if detected=='fall' and frame_number%30 == 0:\n",
        "            file_name = os.path.join(fall_image_folder, f\"fall_{i}.jpg\")\n",
        "            cv2.imwrite(file_name, frame)\n",
        "            i +=1\n",
        "        elif detected == 'crash' and frame_number%3 == 0:\n",
        "            file_name = os.path.join(crash_folder, f\"crash_{j}.jpg\")\n",
        "            cv2.imwrite(file_name, frame)\n",
        "            j +=1\n",
        "        if mask_model:\n",
        "            frame,_ = mask_detection(frame)\n",
        "\n",
        "        output_video.write(frame)\n",
        "        frame_number += 1\n",
        "\n",
        "    # Release the video objects\n",
        "    video.release()\n",
        "    output_video.release()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
